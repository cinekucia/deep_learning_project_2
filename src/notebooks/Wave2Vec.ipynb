{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:55.857353Z",
     "start_time": "2024-04-28T16:20:55.840083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "from settings import ALL_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268826bcc102ff06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T16:19:12.125496Z",
     "start_time": "2024-04-28T16:19:11.935358Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset, labels = torch.load(\"C:/Users/Filip/Desktop/PW/2 semestr/Deep Learning/pro2/MINI_DL_RNN/src/artifacts/speech-waveform-v0/validation.pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eec7734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9656f3e19a5f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T16:19:12.163607Z",
     "start_time": "2024-04-28T16:19:12.126661Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "class SpeechClassifier:\n",
    "    def __init__(self, model_name='facebook/wav2vec2-base-960h', device=None, lr=0.001):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(self.device)\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()  # Using CrossEntropyLoss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def load_data(self, train_path, val_path, batch_size=4):\n",
    "        train_dataset, train_labels = torch.load(train_path)\n",
    "        val_dataset, val_labels = torch.load(val_path)\n",
    "        self.train_loader = DataLoader(TensorDataset(train_dataset, train_labels.squeeze(1)), batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(TensorDataset(val_dataset, val_labels.squeeze(1)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def train_and_validate(self, epochs, accumulation_steps=4):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            self.optimizer.zero_grad()\n",
    "            with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "                for step, (inputs, labels) in enumerate(self.train_loader):\n",
    "                    processed = self.processor(inputs.numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "                    inputs = processed.input_values.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    with autocast():\n",
    "                        outputs = self.model(inputs).logits\n",
    "                        # Taking the mean across the time dimension to collapse the logits to expected format for CrossEntropy\n",
    "                        outputs = outputs.mean(dim=1)\n",
    "                        loss = self.loss_function(outputs, labels) / accumulation_steps\n",
    "\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    total_train_loss += loss.item() * accumulation_steps\n",
    "                    pbar.update(1)\n",
    "\n",
    "            print(f\"Average Training Loss: {total_train_loss / len(self.train_loader)}\")\n",
    "            self.validate()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with tqdm(total=len(self.val_loader), desc=\"Validating\", unit=\"batch\") as pbar:\n",
    "            for inputs, labels in self.val_loader:\n",
    "                processed = self.processor(inputs.numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "                inputs = processed.input_values.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                with torch.no_grad(), autocast():\n",
    "                    outputs = self.model(inputs).logits\n",
    "                    # Collapsing the logits to match the label format\n",
    "                    outputs = outputs.mean(dim=1)\n",
    "                    loss = self.loss_function(outputs, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Validation Loss: {total_val_loss / len(self.val_loader)}\")\n",
    "        self.evaluate_performance(all_labels, all_preds)\n",
    "\n",
    "    def evaluate_performance(self, labels, preds):\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(labels, preds))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(labels, preds))\n",
    "\n",
    "# Example usage:\n",
    "train_path = \"C:/Users/Filip/Desktop/PW/2 semestr/Deep Learning/pro2/MINI_DL_RNN/src/artifacts/speech-waveform-v0/train.pt\"\n",
    "val_path = \"C:/Users/Filip/Desktop/PW/2 semestr/Deep Learning/pro2/MINI_DL_RNN/src/artifacts/speech-waveform-v0/validation.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bcf6142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|██████████| 12570/12570 [12:56<00:00, 16.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.6420255718600114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:29<00:00, 56.01batch/s]\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5976120395705922\n",
      "Confusion Matrix:\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0  261]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  245]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  260]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  264]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  247]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  256]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  257]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  256]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  246]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  260]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   39]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0 4113]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       261\n",
      "           1       0.00      0.00      0.00       245\n",
      "           2       0.00      0.00      0.00       260\n",
      "           3       0.00      0.00      0.00       264\n",
      "           4       0.00      0.00      0.00       247\n",
      "           5       0.00      0.00      0.00       256\n",
      "           6       0.00      0.00      0.00       257\n",
      "           7       0.00      0.00      0.00       256\n",
      "           8       0.00      0.00      0.00       246\n",
      "           9       0.00      0.00      0.00       260\n",
      "          10       0.00      0.00      0.00        39\n",
      "          11       0.61      1.00      0.76      4113\n",
      "\n",
      "    accuracy                           0.61      6704\n",
      "   macro avg       0.05      0.08      0.06      6704\n",
      "weighted avg       0.38      0.61      0.47      6704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 12570/12570 [13:07<00:00, 15.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.560359749532258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:30<00:00, 54.55batch/s]\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5803056958182615\n",
      "Confusion Matrix:\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0  261]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  245]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  260]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  264]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  247]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  256]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  257]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  256]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  246]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  260]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   39]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0 4113]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       261\n",
      "           1       0.00      0.00      0.00       245\n",
      "           2       0.00      0.00      0.00       260\n",
      "           3       0.00      0.00      0.00       264\n",
      "           4       0.00      0.00      0.00       247\n",
      "           5       0.00      0.00      0.00       256\n",
      "           6       0.00      0.00      0.00       257\n",
      "           7       0.00      0.00      0.00       256\n",
      "           8       0.00      0.00      0.00       246\n",
      "           9       0.00      0.00      0.00       260\n",
      "          10       0.00      0.00      0.00        39\n",
      "          11       0.61      1.00      0.76      4113\n",
      "\n",
      "    accuracy                           0.61      6704\n",
      "   macro avg       0.05      0.08      0.06      6704\n",
      "weighted avg       0.38      0.61      0.47      6704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   4%|▍         | 542/12570 [00:33<12:29, 16.06batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m SpeechClassifier()\n\u001b[0;32m      2\u001b[0m classifier\u001b[38;5;241m.\u001b[39mload_data(train_path, val_path)\n\u001b[1;32m----> 3\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain_and_validate(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mSpeechClassifier.train_and_validate\u001b[1;34m(self, epochs, accumulation_steps)\u001b[0m\n\u001b[0;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Taking the mean across the time dimension to collapse the logits to expected format for CrossEntropy\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1963\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;124;03m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;124;03m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1961\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1963\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwav2vec2(\n\u001b[0;32m   1964\u001b[0m     input_values,\n\u001b[0;32m   1965\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1966\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1967\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1968\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1969\u001b[0m )\n\u001b[0;32m   1971\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1972\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1562\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1557\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[0;32m   1558\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[0;32m   1559\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[0;32m   1560\u001b[0m )\n\u001b[1;32m-> 1562\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1563\u001b[0m     hidden_states,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1566\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1567\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1568\u001b[0m )\n\u001b[0;32m   1570\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:806\u001b[0m, in \u001b[0;36mWav2Vec2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    799\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    800\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    801\u001b[0m             hidden_states,\n\u001b[0;32m    802\u001b[0m             attention_mask,\n\u001b[0;32m    803\u001b[0m             output_attentions,\n\u001b[0;32m    804\u001b[0m         )\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m layer(\n\u001b[0;32m    807\u001b[0m             hidden_states, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[0;32m    808\u001b[0m         )\n\u001b[0;32m    809\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:692\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    689\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(hidden_states)\n\u001b[0;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:665\u001b[0m, in \u001b[0;36mWav2Vec2FeedForward.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    662\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_dropout(hidden_states)\n\u001b[0;32m    664\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dense(hidden_states)\n\u001b[1;32m--> 665\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dropout(hidden_states)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = SpeechClassifier()\n",
    "classifier.load_data(train_path, val_path)\n",
    "classifier.train_and_validate(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc32a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Config, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "class SpeechClassifier:\n",
    "    def __init__(self, model_name='facebook/wav2vec2-base-960h', device=None, lr=0.001, num_labels=12):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        \n",
    "        # Load configuration for Wav2Vec2\n",
    "        config = Wav2Vec2Config.from_pretrained(model_name)\n",
    "        config.num_labels = num_labels  # Setting the number of classes\n",
    "        \n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name, config=config)\n",
    "        self.model.to(self.device)\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()  # Using CrossEntropyLoss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def load_data(self, train_path, val_path, batch_size=4):\n",
    "        train_dataset, train_labels = torch.load(train_path)\n",
    "        val_dataset, val_labels = torch.load(val_path)\n",
    "        self.train_loader = DataLoader(TensorDataset(train_dataset, train_labels.squeeze(1)), batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(TensorDataset(val_dataset, val_labels.squeeze(1)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def train_and_validate(self, epochs, accumulation_steps=4):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            train_correct = 0\n",
    "            total_train = 0\n",
    "            with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "                for step, (inputs, labels) in enumerate(self.train_loader):\n",
    "                    processed = self.processor(inputs.numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "                    inputs = processed.input_values.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    with autocast():\n",
    "                        outputs = self.model(inputs).logits[:, -1, :]\n",
    "                        loss = self.loss_function(outputs, labels) / accumulation_steps\n",
    "\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    total_train_loss += loss.item() * accumulation_steps\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    train_correct += torch.sum(preds == labels).item()\n",
    "                    total_train += labels.size(0)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            train_accuracy = train_correct / total_train\n",
    "            print(f\"Average Training Loss: {total_train_loss / len(self.train_loader)}\")\n",
    "            print(f\"Training Accuracy: {train_accuracy}\")\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            print(f\"Validation Loss: {val_loss}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        with tqdm(total=len(self.val_loader), desc=\"Validating\", unit=\"batch\") as pbar:\n",
    "            for inputs, labels in self.val_loader:\n",
    "                processed = self.processor(inputs.numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "                inputs = processed.input_values.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                with torch.no_grad(), autocast():\n",
    "                    outputs = self.model(inputs).logits[:, -1, :]\n",
    "                    loss = self.loss_function(outputs, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_correct += torch.sum(preds == labels).item()\n",
    "                    total_val += labels.size(0)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        val_accuracy = val_correct / total_val\n",
    "        return total_val_loss / len(self.val_loader), val_accuracy\n",
    "\n",
    "    def evaluate_performance(self, total_val, val_correct):\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(total_val.cpu().numpy(), val_correct.cpu().numpy()))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(total_val.cpu().numpy(), val_correct.cpu().numpy()))\n",
    "\n",
    "# Example usage remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c4177da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|██████████| 12570/12570 [12:56<00:00, 16.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.6845521748753511\n",
      "Training Accuracy: 0.6200922868849198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:29<00:00, 56.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.597636002060337\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 12570/12570 [12:40<00:00, 16.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5708410222092222\n",
      "Training Accuracy: 0.6276303751143641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:29<00:00, 56.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.580131184798721\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 12570/12570 [12:40<00:00, 16.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5567031465832355\n",
      "Training Accuracy: 0.6283463940490871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:26<00:00, 64.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5897494659788\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 12570/12570 [11:55<00:00, 17.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5500179591213035\n",
      "Training Accuracy: 0.6287640717610088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:27<00:00, 61.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5923604305148977\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 12570/12570 [11:51<00:00, 17.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.544301914315235\n",
      "Training Accuracy: 0.6289231870798361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:26<00:00, 62.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5801552565399388\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 12570/12570 [11:49<00:00, 17.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.543590589279017\n",
      "Training Accuracy: 0.6290027447392498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:26<00:00, 62.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5789320042139021\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 12570/12570 [11:53<00:00, 17.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5421758232936085\n",
      "Training Accuracy: 0.6287640717610088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:27<00:00, 61.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5827754710340842\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 12570/12570 [11:59<00:00, 17.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5400415189131538\n",
      "Training Accuracy: 0.6290027447392498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:32<00:00, 52.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.578565611190614\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 12570/12570 [13:45<00:00, 15.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5393005249703224\n",
      "Training Accuracy: 0.6289828553243964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:30<00:00, 54.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.575167853963119\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 12570/12570 [12:56<00:00, 16.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5389844823092247\n",
      "Training Accuracy: 0.6290425235689566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 1676/1676 [00:29<00:00, 56.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.577851325060132\n",
      "Validation Accuracy: 0.6135143198090692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = SpeechClassifier()\n",
    "classifier.load_data(train_path, val_path)\n",
    "classifier.train_and_validate(epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
